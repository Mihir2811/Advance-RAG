{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSxJVZw5d4-C",
        "outputId": "0b29512f-e46e-4397-f147-511801fe7697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.0/120.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain-community pdfminer.six langchain-openai faiss-cpu chromadb gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PDFMinerLoader\n",
        "\n",
        "pdf_path = ('attention.pdf')\n",
        "\n",
        "loader = PDFMinerLoader(pdf_path)\n",
        "\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "35Y80gWqeppb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in docs:\n",
        "  print(i.page_content[:500])\n",
        "  print(i.metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enRiVmcmf2Fz",
        "outputId": "6a45047b-40a1-4b05-e738-e2d0bf36dabf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPIL Corporate HR Policies  \n",
            "\n",
            "SIRCA PAINTS INDIA LTD \n",
            "\n",
            "NEW DELHI  \n",
            "\n",
            "CORPORATE  \n",
            "\n",
            "  HUMAN RESOURCES \n",
            "POLICIES & MANUALS\n",
            "\fSPIL Corporate HR Policies  \n",
            "\n",
            "Section 1: Introduction  \n",
            "\n",
            "This handbook is the summary of the policies, procedures, guidance and benefits to the employees \n",
            "and organization. It is an introduction to our vision, mission, values, what you expect from us and \n",
            "what  we  expect  from  you.  We  believe  that  employees  are  the  assets  of  the  organization  and  to \n",
            "understand the\n",
            "{'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2020-08-26T06:56:00+00:00', 'author': 'hr', 'moddate': '2020-08-26T06:56:00+00:00', 'total_pages': 24, 'source': 'attention.pdf'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Text into Chunks\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "\n",
        "# Splitting into Documents\n",
        "documents = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "6A2KdkCUgcau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "BpFwpfNDg85h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector Embedding And Vector Store\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Now, create the Chroma vector store with the filtered documents\n",
        "db = FAISS.from_documents(documents, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "83ycoqnug4bG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "cb5dcbe5-7632-4132-8923-dd0dd0a56ddc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-3955572177.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Now, create the Chroma vector store with the filtered documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOpenAIEmbeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \"\"\"\n\u001b[0;32m-> 1043\u001b[0;31m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m         return cls.__from(\n\u001b[1;32m   1045\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/embeddings/base.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m#       than the maximum context and use length-safe embedding function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         return self._get_len_safe_embeddings(\n\u001b[0m\u001b[1;32m    591\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/embeddings/base.py\u001b[0m in \u001b[0;36m_get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mbatched_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m             response = self.client.create(\n\u001b[0m\u001b[1;32m    479\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_chunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mclient_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         )\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Sirca Co-Founders\"\n",
        "retireved_results1=db.similarity_search(query)\n",
        "query = \"Company’s Vision\"\n",
        "retireved_results2=db.similarity_search(query)"
      ],
      "metadata": {
        "id": "Jz5z9gjNhDq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = db.similarity_search_with_score(query, k=4)"
      ],
      "metadata": {
        "id": "8s1ucc_vh_aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc, score in results:\n",
        "    print(f\"Score: {score}\")\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "immaeaz8jAoZ",
        "outputId": "6d970045-a27e-47d7-9e9f-7358fe99d2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.3349238336086273\n",
            "Content: Sirca Co-Founders  \n",
            "\n",
            "The foundation of the company was laid down by Mr Sanjay Agarwal & Mr Gurjit Singh Bains in the \n",
            "year  2006  with  a  vision  to  have  a  distinct  global  presence  in  Paint  Industry  by  providing  high \n",
            "quality coating and technical assistance which leads to as healthy customer relationship.  \n",
            "\n",
            "Company’s Vision  \n",
            "\n",
            "To  be  one  of  the  most  respectable  brands  in  the  category  through  brand  building  initiatives, \n",
            "providing  world  class  products  with  consistent,  quality,  leading  to  profitability  and  growth  of \n",
            "everyone who is associated with the organization.  \n",
            "\n",
            "Sales Vision : \n",
            "\n",
            "RESPECT  \n",
            "\n",
            "R \n",
            "\n",
            "E \n",
            "\n",
            "S \n",
            "\n",
            "P \n",
            "\n",
            "E \n",
            "\n",
            "C \n",
            "\n",
            "T \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            "Reliability  \n",
            "\n",
            "You can count on us  \n",
            "\n",
            "Excellence  \n",
            "\n",
            "Is our Standard  \n",
            "\n",
            "Service  \n",
            "\n",
            "Customer First and accomplish the needs   \n",
            "\n",
            "People  \n",
            "\n",
            "Serve People with Fairness & Firmness  \n",
            "\n",
            "Empowerment  \n",
            "\n",
            "Enabling each to attain his/her potential  \n",
            "\n",
            "Caring  \n",
            "\n",
            "Care for all as we wish to be cared for  \n",
            "\n",
            "Team work    \n",
            "\n",
            "Foster a spirit of Team work.  \n",
            "\n",
            "Commitment  \n",
            "\n",
            "Sirca  tries  to  provide  the  solutions as  per  the  customer’s  requirement  &  needs  and  also  leads  to \n",
            "innovative and cost saving solution within their total production process. \"Team Sirca” works had \n",
            "to understand their customer’s products and production processes to become their most reliable & \n",
            "dependable, complete and innovative supplier for Wood Coating Solutions. \n",
            "\n",
            "SPIL Ethics Pledge – Word of Honor  \n",
            "\n",
            "\n",
            "---\n",
            "Score: 0.42567697167396545\n",
            "Content: SPIL Ethics Pledge – Word of Honor  \n",
            "\n",
            " \n",
            "\n",
            "It  is  the  responsibility  of  every  employee  to  put  the  efforts  to  achieve  the  higher  productivity \n",
            "and service standards.\n",
            "\f \n",
            " \n",
            "\n",
            " \n",
            "\n",
            "SPIL Corporate HR Policies  \n",
            "\n",
            "  Respect  of  the  core  values,  policies  and  procedures,  manuals  to  achieve  the  goals  of  the \n",
            "\n",
            "organization.  \n",
            "\n",
            "  Proper and maximum utilization of manpower and resources to ensure the Company’s growth.  \n",
            "\n",
            "  Employee will provide the best solution to the customer’s enquires or grievances.  \n",
            "\n",
            "  Team Work and Excellence in work is working culture of the organization \n",
            "\n",
            "Section 3: Recruitment  \n",
            "\n",
            "The  company  policy  on  recruitment  strives  for  equal  opportunity  to  all  irrespective  of  any \n",
            "distinction of gender, sexual orientation, caste or any disable applicants. All appointments will be \n",
            "done by the approval of the Director or Managing Director.  \n",
            "\n",
            "Recruitment strategy of SPIL is strictly based on three points  \n",
            "\n",
            "Talent Acquisition  \n",
            "\n",
            "Talent Management  \n",
            "\n",
            "Talent Development  \n",
            "\n",
            "Talent Acquisition:  GROWTH  \n",
            "\n",
            "G: GREAT, \n",
            "\n",
            "R:Reward,    \n",
            "\n",
            "O: Opportunity,  \n",
            "\n",
            "W:Work,  \n",
            "\n",
            "T: Team \n",
            "\n",
            "H: Helpful  \n",
            "\n",
            "GREAT REWARD AND OPPORTUNITY OF WORK WITH TEAM AND HELPFUL ATMOSPHERE  \n",
            "\n",
            "This policy covers the vacant position on PAN India basis across the functions, department, level, \n",
            "grade and hierarchy. The following steps are to be followed to hire the candidate\n",
            "---\n",
            "Score: 0.460747629404068\n",
            "Content: anywhere in India or Abroad.  \n",
            "\n",
            "b)  “Board” means the “Board of Directors” of SPIL and it includes all Committee of Directors.  \n",
            "\n",
            "c)  “Approving Authority” means the management/higher authority i.e. Managing Director/Director \n",
            "\n",
            "of the Company.  \n",
            "\n",
            "d)  “Employee” means full time employment/retainership/interns/apprentice/Trainees or any other \n",
            "\n",
            "employee who is working with SPIL.  \n",
            "\n",
            "e)  “Dependents”  means  the  employees  family  dependents.  Family  includes  employee  mother, \n",
            "father, spouse and children. The children age of above 24 will not be included as a Dependent \n",
            "Children.  \n",
            "\n",
            "f)  “Year” means the financial year i.e. April to March  \n",
            "\n",
            "g)  “Base City” means the native location, a permanent residence of the employee.  \n",
            "\n",
            "h)  “Posted City” means the work place where the employee will be posted at the time of joining.  \n",
            "\n",
            "i)  “Malfeasance”  means  official  misconduct,  an  unlawful  act  which  affects  the  performance  of \n",
            "\n",
            "official duties.\n",
            "\fSPIL Corporate HR Policies  \n",
            "\n",
            "Section 2: Company Profile \n",
            "\n",
            "SPIL  is  a  company  engaged  in  marketing  and  trading/distribution  of  wood  coatings  and  allied \n",
            "products.  It  is  the  first  company  to  launch  wood  filler  in  India  and  opened  the  branches  on  PAN \n",
            "INDIA  basis.  Sirca  Paints  specialize  in  the  production  of  coating  for  wood.  This  living  material, \n",
            "which  is  increasingly  valuable  and  essential  in  ecological  terms  deserves  the  best  possible \n",
            "optimization and protection.\n",
            "---\n",
            "Score: 0.46312737464904785\n",
            "Content: Guard/Covers or any other mobile accessories.  \n",
            "\n",
            "b)  In  case  of  loss of  Cell  Phone,  the  depreciated  value of  the  cell phone  shall be debited  to  the \n",
            "\n",
            "employee.  \n",
            "\n",
            "c)  In case of Travel Abroad, International Roaming/ISD facilities will be applicable to Zonal Sales \n",
            "Manager  and  above  and  for  any  other  grades  the  Competent  Authority  Approval  will  be \n",
            "required.  \n",
            "\n",
            "d)  All Employees are eligible for the official Sim. The Reporting Manager will send the email to the \n",
            "Administration Department for the Sim card. The Employee will collect the official sim within \n",
            "three working days after the email received from the Reporting Manager.\n",
            "\fSPIL Corporate HR Policies  \n",
            "\n",
            "Section 17: Performance Incentive \n",
            "\n",
            "The organization main motive for the Performance Incentive Policy is to achieve the outcomes and \n",
            "also  to  increase  the  work  productivity.  This  policy  goal  is  to  encourage  their  employee  to  adopt \n",
            "healthier behavior through company engagement program.  \n",
            "\n",
            "The performance is divided into three categories  \n",
            "\n",
            "a)  Financial Incentives \n",
            "\n",
            "b)  Recognition based Incentives  \n",
            "\n",
            "c)  Specialty Awards\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(retireved_results1):\n",
        "    print(f\"Result {i+1}:\")\n",
        "    print(\"Page Content:\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\nMetadata:\")\n",
        "    print(doc.metadata)\n",
        "    print(\"-\" * 20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVN-6PPqhRKZ",
        "outputId": "ae462bd6-6e44-4efe-c340-c9828fff2ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result 1:\n",
            "Page Content:\n",
            "Sirca Co-Founders  \n",
            "\n",
            "The foundation of the company was laid down by Mr Sanjay Agarwal & Mr Gurjit Singh Bains in the \n",
            "year  2006  with  a  vision  to  have  a  distinct  global  presence  in  Paint  Industry  by  providing  high \n",
            "quality coating and technical assistance which leads to as healthy customer relationship.  \n",
            "\n",
            "Company’s Vision  \n",
            "\n",
            "To  be  one  of  the  most  respectable  brands  in  the  category  through  brand  building  initiatives, \n",
            "providing  world  class  products  with  consistent,  quality,  leading  to  profitability  and  growth  of \n",
            "everyone who is associated with the organization.  \n",
            "\n",
            "Sales Vision : \n",
            "\n",
            "RESPECT  \n",
            "\n",
            "R \n",
            "\n",
            "E \n",
            "\n",
            "S \n",
            "\n",
            "P \n",
            "\n",
            "E \n",
            "\n",
            "C \n",
            "\n",
            "T \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            ": \n",
            "\n",
            "Reliability  \n",
            "\n",
            "You can count on us  \n",
            "\n",
            "Excellence  \n",
            "\n",
            "Is our Standard  \n",
            "\n",
            "Service  \n",
            "\n",
            "Customer First and accomplish the needs   \n",
            "\n",
            "People  \n",
            "\n",
            "Serve People with Fairness & Firmness  \n",
            "\n",
            "Empowerment  \n",
            "\n",
            "Enabling each to attain his/her potential  \n",
            "\n",
            "Caring  \n",
            "\n",
            "Care for all as we wish to be cared for  \n",
            "\n",
            "Team work    \n",
            "\n",
            "Foster a spirit of Team work.  \n",
            "\n",
            "Commitment  \n",
            "\n",
            "Sirca  tries  to  provide  the  solutions as  per  the  customer’s  requirement  &  needs  and  also  leads  to \n",
            "innovative and cost saving solution within their total production process. \"Team Sirca” works had \n",
            "to understand their customer’s products and production processes to become their most reliable & \n",
            "dependable, complete and innovative supplier for Wood Coating Solutions. \n",
            "\n",
            "SPIL Ethics Pledge – Word of Honor  \n",
            "\n",
            "\n",
            "\n",
            "Metadata:\n",
            "{'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2020-08-26T06:56:00+00:00', 'author': 'hr', 'moddate': '2020-08-26T06:56:00+00:00', 'total_pages': 24, 'source': 'attention.pdf'}\n",
            "--------------------\n",
            "Result 2:\n",
            "Page Content:\n",
            "c)  Each  Employee  is  expected  to  obey  the  rules  and  regulations  abide  by  the  Management \n",
            "\n",
            "related to the security purpose.  \n",
            "\n",
            "d)  All  Employees  are  responsible  for  their  own  personal  belonging  and  properties  left  in  office. \n",
            "The Company assumes no liabilities for any loss or damage   to  personal  belonging  and \n",
            "property.  \n",
            "\n",
            "e)  The office space, equipment, material and other properties shall be used only for Sirca Paints \n",
            "India Limited (SPIL). Employee who uses the company property like : Laptop, Mobile Phones, \n",
            "Camera,  Projectors  and  other  material  are  responsible  for  the  sake  keeping  of  these \n",
            "equipment.  \n",
            "\n",
            "f)  To make the safety and security of the employees of Sirca, only authorize vendors are allowed \n",
            "to  visit  to  the  workplace.  All  vendors are  authorized  to  enter  through  the  main  lift  area and \n",
            "wait  at  the  main  reception  area  (if  required).  The  proper  visitor  ID  card  will  be  given  to  the \n",
            "visitors before entering to the office.  \n",
            "\n",
            "Section 9: Transfer Provision  \n",
            "\n",
            "a) \n",
            "\n",
            "b) \n",
            "\n",
            "c) \n",
            "\n",
            "The  company  has  to  right  to  transfer  you  with  immediate  effect either  in  the  subsidiary  or \n",
            "associate of SIRCA or any location.  \n",
            "\n",
            "In case of transfer of location, the employee is eligible to take the leave of 4 days as “Special \n",
            "Leave” by said leave; the employee can make the arrangement of shifting and resettlement.\n",
            "\n",
            "Metadata:\n",
            "{'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2020-08-26T06:56:00+00:00', 'author': 'hr', 'moddate': '2020-08-26T06:56:00+00:00', 'total_pages': 24, 'source': 'attention.pdf'}\n",
            "--------------------\n",
            "Result 3:\n",
            "Page Content:\n",
            "SPIL Corporate HR Policies  \n",
            "\n",
            "SIRCA PAINTS INDIA LTD \n",
            "\n",
            "NEW DELHI  \n",
            "\n",
            "CORPORATE  \n",
            "\n",
            "  HUMAN RESOURCES \n",
            "POLICIES & MANUALS\n",
            "\fSPIL Corporate HR Policies  \n",
            "\n",
            "Section 1: Introduction  \n",
            "\n",
            "This handbook is the summary of the policies, procedures, guidance and benefits to the employees \n",
            "and organization. It is an introduction to our vision, mission, values, what you expect from us and \n",
            "what  we  expect  from  you.  We  believe  that  employees  are  the  assets  of  the  organization  and  to \n",
            "understand them the positive work environment play an important role. \n",
            "\n",
            "This Employee Hand Book(EHB) is the confidential property of Sirca Paints India Limited (SPIL) \n",
            "and  any  use,  distributing,  copying  or  disclosure  by  any  person  to  outsiders  without  any  proper \n",
            "authorization is strictly prohibited. \n",
            "\n",
            "Any  query  or  doubt  concerning  the  content  of  the  EHB  should  be  forwarded  to  the  Human \n",
            "Resources Department of SPIL.  \n",
            "\n",
            "Applicability  \n",
            "\n",
            "This  EHB  will  be  applicable  to  the  employees  working  in  Sirca  Paints  India  Limited  (SPIL)  w.e.f \n",
            "August 21, 2020. This book contains all the notices/circulars/extracts/meetings circulated earlier \n",
            "before the date of validity of this handbook.  \n",
            "\n",
            "Definitions  \n",
            "\n",
            "a)  “Company” means Sirca Paints India Limited (SPIL) and will its branches, offices/plants located \n",
            "\n",
            "anywhere in India or Abroad.  \n",
            "\n",
            "b)  “Board” means the “Board of Directors” of SPIL and it includes all Committee of Directors.\n",
            "\n",
            "Metadata:\n",
            "{'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2020-08-26T06:56:00+00:00', 'author': 'hr', 'moddate': '2020-08-26T06:56:00+00:00', 'total_pages': 24, 'source': 'attention.pdf'}\n",
            "--------------------\n",
            "Result 4:\n",
            "Page Content:\n",
            "anywhere in India or Abroad.  \n",
            "\n",
            "b)  “Board” means the “Board of Directors” of SPIL and it includes all Committee of Directors.  \n",
            "\n",
            "c)  “Approving Authority” means the management/higher authority i.e. Managing Director/Director \n",
            "\n",
            "of the Company.  \n",
            "\n",
            "d)  “Employee” means full time employment/retainership/interns/apprentice/Trainees or any other \n",
            "\n",
            "employee who is working with SPIL.  \n",
            "\n",
            "e)  “Dependents”  means  the  employees  family  dependents.  Family  includes  employee  mother, \n",
            "father, spouse and children. The children age of above 24 will not be included as a Dependent \n",
            "Children.  \n",
            "\n",
            "f)  “Year” means the financial year i.e. April to March  \n",
            "\n",
            "g)  “Base City” means the native location, a permanent residence of the employee.  \n",
            "\n",
            "h)  “Posted City” means the work place where the employee will be posted at the time of joining.  \n",
            "\n",
            "i)  “Malfeasance”  means  official  misconduct,  an  unlawful  act  which  affects  the  performance  of \n",
            "\n",
            "official duties.\n",
            "\fSPIL Corporate HR Policies  \n",
            "\n",
            "Section 2: Company Profile \n",
            "\n",
            "SPIL  is  a  company  engaged  in  marketing  and  trading/distribution  of  wood  coatings  and  allied \n",
            "products.  It  is  the  first  company  to  launch  wood  filler  in  India  and  opened  the  branches  on  PAN \n",
            "INDIA  basis.  Sirca  Paints  specialize  in  the  production  of  coating  for  wood.  This  living  material, \n",
            "which  is  increasingly  valuable  and  essential  in  ecological  terms  deserves  the  best  possible \n",
            "optimization and protection.\n",
            "\n",
            "Metadata:\n",
            "{'producer': 'www.ilovepdf.com', 'creator': 'Microsoft® Word 2016', 'creationdate': '2020-08-26T06:56:00+00:00', 'author': 'hr', 'moddate': '2020-08-26T06:56:00+00:00', 'total_pages': 24, 'source': 'attention.pdf'}\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling the LLM Model and Buffer Memory\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "llm = ChatOpenAI(model='gpt-3.5-turbo')\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0KDmDbhhSfE",
        "outputId": "c0d58f4b-dd6e-426e-ced2-03c0b278f97d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x78598adad5d0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x78598adadb50>, root_client=<openai.OpenAI object at 0x78598adacf50>, root_async_client=<openai.AsyncOpenAI object at 0x785999eb8ed0>, model_kwargs={}, openai_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Answer the Question:** Based *only* on the provided context, answer the following question.\n",
        "2.  **Cite Sources:** For each piece of information in your answer, cite the source from the context.\n",
        "3.  **Confidence Score:** Provide a confidence score (out of 100) for your answer.\n",
        "4.  **Reasoning:** Explain your reasoning for the answer and the confidence score.\n",
        "5.  **Guardrails:**\n",
        "    *   Do not answer questions that are not related to the context.\n",
        "    *   If the answer is not found in the context, state that and do not provide an answer.\n",
        "    *   Do not generate any harmful, unethical, or offensive content.\n",
        "\n",
        "**Context:**\n",
        "\n",
        "<context>\n",
        "\n",
        "{context}\n",
        "\n",
        "</context>\n",
        "\n",
        "**Question:** {input}\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Confidence Score:**\n",
        "\n",
        "**Reasoning:**\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "J1_inRWSkWz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create Stuff Docment Chain\n",
        "\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "document_chain=create_stuff_documents_chain(llm,prompt)"
      ],
      "metadata": {
        "id": "o5xdaLwJkuze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever=db.as_retriever()\n",
        "retriever"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ-Er2Xwk19W",
        "outputId": "cc3a4ff5-066b-4c43-f3f0-41361a69faa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x78598aba4450>, search_kwargs={})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "document_chain=create_stuff_documents_chain(\n",
        "    llm,\n",
        "    prompt)\n",
        "\n",
        "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n",
        "\n",
        "response1 = retrieval_chain.invoke({\"input\":\"Introduction\"})\n",
        "\n",
        "response2 = retrieval_chain.invoke({\"input\":\"Company Vision\"})\n",
        "\n",
        "print(response1['answer'])\n",
        "\n",
        "response2['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "smYie8qdk3ev",
        "outputId": "46b736a1-0f45-49f2-8da0-b315be38519e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It seems that the question is not related to the context provided. No relevant information is available to provide an answer to this question.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"**Answer:** The company's vision is to be one of the most respectable brands in the category through brand-building initiatives and providing world-class products with consistent quality, leading to profitability and growth for everyone associated with the organization.\\n\\n**Confidence Score:** 95\\n\\n**Reasoning:** The provided context explicitly states the company's vision as described above, which is supported by the quotes from the company's vision section. The information is clear and direct, making the answer highly confident.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = retrieval_chain.invoke({\"input\":\"Company’s Vision \"})"
      ],
      "metadata": {
        "id": "rDy3ZzyEmnWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input Query:\", response2['input'])\n",
        "\n",
        "print(\"\\n--- Answer ---\")\n",
        "\n",
        "print(response2['answer'])\n",
        "\n",
        "print(\"\\n--- Context Documents ---\")\n",
        "\n",
        "'''for i, doc in enumerate(response2['context']):\n",
        "    print(f\"\\n--- Document {i+1} ---\")\n",
        "    print(\"Page Content:\")\n",
        "    print(doc.page_content)\n",
        "    print(\"\\nMetadata:\")\n",
        "    print(doc.metadata)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "c7StKTsbmw_G",
        "outputId": "9a9d4d60-d9cc-48f2-eb03-cc29eb47704a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Query: Company’s Vision \n",
            "\n",
            "--- Answer ---\n",
            "**Answer:** The company's vision is to be one of the most respectable brands in the category through brand building initiatives, providing world-class products with consistent quality, leading to profitability and growth of everyone associated with the organization.\n",
            "\n",
            "**Confidence Score:** 95%\n",
            "\n",
            "**Reasoning:** The information about the company's vision is explicitly provided in the context under the section \"Company’s Vision.\" The text clearly outlines the vision goals of the company, emphasizing brand building, quality products, profitability, and growth for all associated with the organization. The wording and clarity of this vision statement make it a direct and confident answer.\n",
            "\n",
            "--- Context Documents ---\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for i, doc in enumerate(response2[\\'context\\']):\\n    print(f\"\\n--- Document {i+1} ---\")\\n    print(\"Page Content:\")\\n    print(doc.page_content)\\n    print(\"\\nMetadata:\")\\n    print(doc.metadata)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Answer ---\n",
        "\n",
        "**Answer:** The vision of the company, Sirca, is to be one of the most respectable brands in its category by taking initiatives for brand building, providing world-class products consistently with good quality, leading to profitability, and growth for everyone who is associated with the organization.\n",
        "\n",
        "**Confidence Score:** 100\n",
        "\n",
        "**Reasoning:** The company's vision is explicitly spelled out in the provided text. It states directly, \"To be one of the most respectable brands in the category through brand building initiatives, providing world class products with consistent, quality, leading to profitability and growth of everyone who is associated with the organization.\" Hence, the confidence score is 100.\n"
      ],
      "metadata": {
        "id": "FqWjGCVKqKd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Interface"
      ],
      "metadata": {
        "id": "MfcdUYKmD2LT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def answer(prompt):\n",
        "\n",
        "    response = retrieval_chain.invoke({\"input\":prompt})\n",
        "    return response['answer']\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=answer,\n",
        "    inputs=gr.Textbox(placeholder=\"Enter your question here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG Application\",\n",
        "    description=\"Ask questions about your document and get answers.\",\n",
        "    )\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "ii-ZPqCcD4EO",
        "outputId": "bc144fda-e79f-4588-adb6-27be8b02b89f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://21964728e760b69638.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://21964728e760b69638.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Memory to RAG"
      ],
      "metadata": {
        "id": "BG7NQzYnMp2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Answer the Question:** Based *only* on the provided context, answer the following question.\n",
        "2.  **Cite Sources:** For each piece of information in your answer, cite the source from the context.\n",
        "3.  **Confidence Score:** Provide a confidence score (out of 100) for your answer.\n",
        "4.  **Reasoning:** Explain your reasoning for the answer and the confidence score.\n",
        "5.  **Guardrails:**\n",
        "    *   Do not answer questions that are not related to the context.\n",
        "    *   If the answer is not found in the context, state that and do not provide an answer.\n",
        "    *   Do not generate any harmful, unethical, or offensive content.\n",
        "\n",
        "**Chat History:**\n",
        "{chat_history}\n",
        "\n",
        "**Context:**\n",
        "\n",
        "<context>\n",
        "\n",
        "{context}\n",
        "\n",
        "</context>\n",
        "\n",
        "**Question:** {question}\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Confidence Score:**\n",
        "\n",
        "**Reasoning:**\n",
        "\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "xxjeQ5B3K4ya",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae8610b-6c39-41e3-eaf7-4969f5435a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-20-1181526916.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=db.as_retriever(),\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        ")"
      ],
      "metadata": {
        "id": "C2MoxzbuK_hT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = retrieval_chain.invoke({\"question\": \"company vision\"})\n",
        "print(response['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAurPm5wLBAK",
        "outputId": "e5136764-2ed9-4d1f-f6a0-6ccb0a7ad0f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Answer:** The company's vision is to be one of the most respectable brands in the category through brand-building initiatives, providing world-class products with consistent quality, leading to profitability and growth of everyone associated with the organization.\n",
            "\n",
            "**Confidence Score:** 95\n",
            "\n",
            "**Reasoning:** The answer is directly extracted from the context provided where the company's vision is clearly outlined. The information is specific and detailed, leading to a high confidence score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = retrieval_chain.invoke({\"question\": \"company hiring\"})\n",
        "print(response2['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0VACJURLEK8",
        "outputId": "361a1111-2377-4856-a25e-ad3a1212c810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Answer:** The company's approach to hiring new employees involves providing detailed job descriptions to the HR Department, getting approval from the Director/Managing Director, and utilizing various recruitment sources such as Employment Sites, Internal Job Postings, Referral Candidate Schemes, Campus Recruitment, and Placement Agencies. The recruitment process includes shortlisting based on job description criteria and academic qualifications, internship programs, campus placements, and inductions that cover organizational policies, vision, and mission.\n",
            "\n",
            "**Confidence Score:** 90\n",
            "\n",
            "**Reasoning:** The information is directly sourced from the context provided, detailing the steps involved in the company's approach to hiring new employees. The process outlined is specific and comprehensive, leading to a high confidence score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5u_Nj2oMDhq",
        "outputId": "3483668b-b2d5-43f8-c16a-0ac4188b8006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['question', 'chat_history', 'answer'])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response['chat_history']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtQaUhmRMYLz",
        "outputId": "8de3c17d-16dd-4643-bcdd-cf84a4b6f574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='company vision', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"**Answer:** The company's vision is to be one of the most respectable brands in the category through brand-building initiatives, providing world-class products with consistent quality, leading to profitability and growth of everyone associated with the organization.\\n\\n**Confidence Score:** 95\\n\\n**Reasoning:** The answer is directly extracted from the context provided where the company's vision is clearly outlined. The information is specific and detailed, leading to a high confidence score.\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='company hiring', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"**Answer:** The company's approach to hiring new employees involves providing detailed job descriptions to the HR Department, getting approval from the Director/Managing Director, and utilizing various recruitment sources such as Employment Sites, Internal Job Postings, Referral Candidate Schemes, Campus Recruitment, and Placement Agencies. The recruitment process includes shortlisting based on job description criteria and academic qualifications, internship programs, campus placements, and inductions that cover organizational policies, vision, and mission.\\n\\n**Confidence Score:** 90\\n\\n**Reasoning:** The information is directly sourced from the context provided, detailing the steps involved in the company's approach to hiring new employees. The process outlined is specific and comprehensive, leading to a high confidence score.\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fully Working Clean Version**"
      ],
      "metadata": {
        "id": "1a9hoK28QHOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploaded File RAG"
      ],
      "metadata": {
        "id": "wSPOZ7Ht9Ara"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Advanced RAG with LLM + Memory\"\"\"\n",
        "\n",
        "# Install Required Libraries\n",
        "\n",
        "#!pip install langchain-community\n",
        "#!pip install pdfminer.six\n",
        "#!pip install langchain-openai\n",
        "#!pip install faiss-cpu\n",
        "#!pip install gradio\n",
        "\n",
        "# 1. Load PDF and Split into Chunks\n",
        "from langchain.document_loaders import PDFMinerLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "pdf_path = \"attention.pdf\"\n",
        "loader = PDFMinerLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "documents = text_splitter.split_documents(docs)\n",
        "\n",
        "# 2. Set OpenAI API Key\n",
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# 3. Create Vector Store (FAISS + OpenAI Embeddings)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "db = FAISS.from_documents(documents, embedding)\n",
        "\n",
        "# 4. Initialize LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# 5. Create Prompt Template\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Answer the Question:** Based *only* on the provided context, answer the following question.\n",
        "2.  **Cite Sources:** For each piece of information in your answer, cite the source from the context.\n",
        "3.  **Confidence Score:** Provide a confidence score (out of 100) for your answer.\n",
        "4.  **Reasoning:** Explain your reasoning for the answer and the confidence score.\n",
        "5.  **Guardrails:**\n",
        "    *   Do not answer questions that are not related to the context.\n",
        "    *   If the answer is not found in the context, state that and do not provide an answer.\n",
        "    *   Do not generate any harmful, unethical, or offensive content.\n",
        "\n",
        "**Chat History:**\n",
        "{chat_history}\n",
        "\n",
        "**Context:**\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "**Question:** {question}\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Confidence Score:**\n",
        "\n",
        "**Reasoning:**\n",
        "\"\"\")\n",
        "\n",
        "# 6. Add Memory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# 7. Create Conversational Retrieval Chain\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=db.as_retriever(),\n",
        "    memory=memory,\n",
        "    combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        ")\n",
        "\n",
        "# 8. Optional: Test It Manually\n",
        "response = retrieval_chain.invoke({\"question\": \"What is the company’s vision?\"})\n",
        "print(\"Answer:\", response[\"answer\"])\n",
        "\n",
        "# 9. Create Gradio Interface\n",
        "import gradio as gr\n",
        "\n",
        "def answer(user_input):\n",
        "    response = retrieval_chain.invoke({\"question\": user_input})\n",
        "    return response['answer']\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=answer,\n",
        "    inputs=gr.Textbox(placeholder=\"Ask a question about the document...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG with Memory\",\n",
        "    description=\"Ask questions and carry on a conversation about the document.\"\n",
        ")\n",
        "\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "XeUFQ8r4QFlg",
        "outputId": "cadc49ea-bce3-4db0-dbae-532a2fdfe951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: The company's vision is to be one of the most respectable brands in the category through brand building initiatives, providing world-class products with consistent quality, leading to profitability and growth of everyone who is associated with the organization.\n",
            "\n",
            "**Confidence Score:** 100\n",
            "\n",
            "**Reasoning:** The vision is clearly stated in the provided company context.\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://014c295dee9a779bb6.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://014c295dee9a779bb6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **File Upload Option with Qdrant**"
      ],
      "metadata": {
        "id": "7hOAEKD287aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries\n",
        "\n",
        "!pip install langchain-community --quiet\n",
        "!pip install pdfminer.six --quiet\n",
        "!pip install langchain-openai --quiet\n",
        "!pip install qdrant-client --quiet\n",
        "!pip install langchain-qdrant --quiet\n",
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "id": "wMQezxRz6eN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Advanced RAG with LLM + Memory\"\"\"\n",
        "\n",
        "# 1. Load PDF and Split into Chunks\n",
        "from langchain.document_loaders import PDFMinerLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 2. Set OpenAI API Key\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = key\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH')\n",
        "os.environ['LANGSMITH_TRACING']=\"true\"\n",
        "os.environ['LANGSMITH_PROJECT']=\"Advance RAG with LLM\"\n",
        "\n",
        "# 3. Create Vector Store (Qdrant + OpenAI Embeddings)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_qdrant import Qdrant\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# 4. Initialize LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# 5. Create Prompt Template\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Answer the Question:** Based *only* on the provided context, answer the following question.\n",
        "2.  **Cite Sources:** For each piece of information in your answer, cite the source from the context.\n",
        "3.  **Confidence Score:** Provide a confidence score (out of 100) for your answer.\n",
        "4.  **Reasoning:** Explain your reasoning for the answer and the confidence score.\n",
        "5.  **Guardrails:**\n",
        "    *   Do not answer questions that are not related to the context.\n",
        "    *   If the answer is not found in the context, state that and do not provide an answer.\n",
        "    *   Do not generate any harmful, unethical, or offensive content.\n",
        "\n",
        "**Chat History:**\n",
        "{chat_history}\n",
        "\n",
        "**Context:**\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "**Question:** {question}\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Confidence Score:**\n",
        "\n",
        "**Reasoning:**\n",
        "\"\"\")\n",
        "\n",
        "# 6. Add Memory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "# 9. Create Gradio Interface\n",
        "import gradio as gr\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "def answer(user_input, file):\n",
        "    if file is None:\n",
        "        return \"Please upload a PDF file.\"\n",
        "\n",
        "    pdf_path = file.name\n",
        "    loader = PDFMinerLoader(pdf_path)\n",
        "    docs = loader.load()\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "    documents = text_splitter.split_documents(docs)\n",
        "\n",
        "    qdrant = Qdrant.from_documents(\n",
        "        documents,\n",
        "        embedding,\n",
        "        location=\":memory:\",  # Local mode with in-memory storage only\n",
        "        collection_name=\"my_documents\",\n",
        "    )\n",
        "\n",
        "    retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=qdrant.as_retriever(),\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        "    )\n",
        "\n",
        "    response = retrieval_chain.invoke({\"question\": user_input})\n",
        "    return response['answer']\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=answer,\n",
        "    inputs=[gr.Textbox(placeholder=\"Ask a question about the document...\"), gr.File(label=\"Upload PDF\")],\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG with Memory\",\n",
        "    description=\"Ask questions and carry on a conversation about the document.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "BevdGaGrYMnY",
        "outputId": "6aeefdd9-4ef4-42f7-8ab4-4eddfcea25c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-3665300439.py:60: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f5313da4f9aa2137ba.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f5313da4f9aa2137ba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Done Only Once"
      ],
      "metadata": {
        "id": "g5EJMiLh83s8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Advanced RAG with LLM + Memory\"\"\"\n",
        "\n",
        "# 1. Load PDF and Split into Chunks\n",
        "from langchain.document_loaders import PDFMinerLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 2. Set OpenAI API Key\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = key\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH')\n",
        "os.environ['LANGSMITH_TRACING']=\"true\"\n",
        "os.environ['LANGSMITH_PROJECT']=\"Advance RAG with LLM\"\n",
        "\n",
        "# 3. Create Vector Store (Qdrant + OpenAI Embeddings)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_qdrant import Qdrant\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# 4. Initialize LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "# 5. Create Prompt Template\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Answer the Question:** Based *only* on the provided context, answer the following question.\n",
        "2.  **Cite Sources:** For each piece of information in your answer, cite the source from the context.\n",
        "3.  **Confidence Score:** Provide a confidence score (out of 100) for your answer.\n",
        "4.  **Reasoning:** Explain your reasoning for the answer and the confidence score.\n",
        "5.  **Guardrails:**\n",
        "    *   Do not answer questions that are not related to the context.\n",
        "    *   If the answer is not found in the context, state that and do not provide an answer.\n",
        "    *   Do not generate any harmful, unethical, or offensive content.\n",
        "\n",
        "**Chat History:**\n",
        "{chat_history}\n",
        "\n",
        "**Context:**\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "**Question:** {question}\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Confidence Score:**\n",
        "\n",
        "**Reasoning:**\n",
        "\"\"\")\n",
        "\n",
        "# 6. Add Memory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "# 9. Create Gradio Interface\n",
        "import gradio as gr\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "retrieval_chain = None\n",
        "last_file = None\n",
        "\n",
        "def answer(user_input, file):\n",
        "    global retrieval_chain, last_file\n",
        "    if file is None:\n",
        "        return \"Please upload a PDF file.\"\n",
        "\n",
        "    if file.name != last_file:\n",
        "        pdf_path = file.name\n",
        "        loader = PDFMinerLoader(pdf_path)\n",
        "        docs = loader.load()\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "        documents = text_splitter.split_documents(docs)\n",
        "\n",
        "        qdrant = Qdrant.from_documents(\n",
        "            documents,\n",
        "            embedding,\n",
        "            location=\":memory:\",  # Local mode with in-memory storage only\n",
        "            collection_name=\"my_documents\",\n",
        "        )\n",
        "\n",
        "        retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=qdrant.as_retriever(),\n",
        "            memory=memory,\n",
        "            combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        "        )\n",
        "        last_file = file.name\n",
        "\n",
        "    response = retrieval_chain.invoke({\"question\": user_input})\n",
        "    return response['answer']\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=answer,\n",
        "    inputs=[gr.Textbox(placeholder=\"Ask a question about the document...\"), gr.File(label=\"Upload PDF\")],\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG with Memory\",\n",
        "    description=\"Ask questions and carry on a conversation about the document.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "aJnPKnXO74jV",
        "outputId": "a97659d9-9e68-45ba-fca9-f0d751ac073d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1ec660fc99e13ae522.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1ec660fc99e13ae522.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final 0.5"
      ],
      "metadata": {
        "id": "qNRcewkfSsIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Advanced RAG with LLM + Memory\"\"\"\n",
        "\n",
        "# 1. Load PDF and Split into Chunks\n",
        "from langchain.document_loaders import PDFMinerLoader, CSVLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 2. Set OpenAI API Key\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = key\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH')\n",
        "os.environ['LANGSMITH_TRACING']=\"true\"\n",
        "os.environ['LANGSMITH_PROJECT']=\"Advance RAG with LLM\"\n",
        "\n",
        "# 3. Create Vector Store (Qdrant + OpenAI Embeddings)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_qdrant import Qdrant\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# 4. Initialize LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# 5. Create Prompt Template\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Answer the Question:** Based *only* on the provided context, answer the following question.\n",
        "2.  **Cite Sources:** For each piece of information in your answer, cite the source from the context.\n",
        "3.  **Confidence Score:** Provide a confidence score (out of 100) for your answer.\n",
        "4.  **Reasoning:** Explain your reasoning for the answer and the confidence score.\n",
        "5.  **Guardrails:**\n",
        "    *   Do not answer questions that are not related to the context.\n",
        "    *   If the answer is not found in the context, state that and do not provide an answer.\n",
        "    *   Do not generate any harmful, unethical, or offensive content.\n",
        "\n",
        "**Chat History:**\n",
        "{chat_history}\n",
        "\n",
        "**Context:**\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "**Question:** {question}\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Confidence Score:**\n",
        "\n",
        "**Reasoning:**\n",
        "\"\"\")\n",
        "\n",
        "# 6. Add Memory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "# 9. Create Gradio Interface\n",
        "import gradio as gr\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "retrieval_chain = None\n",
        "last_file = None\n",
        "\n",
        "def answer(user_input, file):\n",
        "    global retrieval_chain, last_file\n",
        "    if file is None:\n",
        "        return \"Please upload a PDF or CSV file.\"\n",
        "\n",
        "    if file.name != last_file:\n",
        "        file_path = file.name\n",
        "        if file_path.endswith(\".pdf\"):\n",
        "            loader = PDFMinerLoader(file_path)\n",
        "        elif file_path.endswith(\".csv\"):\n",
        "            loader = CSVLoader(file_path)\n",
        "        else:\n",
        "            return \"Unsupported file type. Please upload a PDF or CSV file.\"\n",
        "\n",
        "        docs = loader.load()\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
        "        documents = text_splitter.split_documents(docs)\n",
        "\n",
        "        qdrant = Qdrant.from_documents(\n",
        "            documents,\n",
        "            embedding,\n",
        "            location=\":memory:\",  # Local mode with in-memory storage only\n",
        "            collection_name=\"my_documents\",\n",
        "        )\n",
        "\n",
        "        retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=qdrant.as_retriever(),\n",
        "            memory=memory,\n",
        "            combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        "        )\n",
        "        last_file = file.name\n",
        "\n",
        "    response = retrieval_chain.invoke({\"question\": user_input})\n",
        "    return response['answer']\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=answer,\n",
        "    inputs=[gr.Textbox(placeholder=\"Ask a question about the document...\"), gr.File(label=\"Upload PDF or CSV\")],\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG with Memory\",\n",
        "    description=\"Ask questions and carry on a conversation about the document.\"\n",
        ")\n",
        "\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "ZwEcXICJz3Wy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "3c9da8f4-afd6-41d8-d964-503e8ac4c20f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bc624f7eeb2a09dc3f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bc624f7eeb2a09dc3f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final RAG\n",
        "\n",
        "RAG-Powered Conversational AI\n",
        "\n",
        "1.   Multi-Format File Support\n",
        "2.   Vector Embeddings\n",
        "3.   Context-Aware Conversations\n",
        "4.   Source Attribution\n",
        "5.   LangSmith Integration\n",
        "6.   User-Friendly Interface\n",
        "7.   Temporary File Handling"
      ],
      "metadata": {
        "id": "xQHEGVk_AE3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Required Libraries\n",
        "\n",
        "!pip install langchain-community --quiet\n",
        "!pip install pdfminer.six --quiet\n",
        "!pip install langchain-openai --quiet\n",
        "!pip install qdrant-client --quiet\n",
        "!pip install langchain-qdrant --quiet\n",
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVpaGEjD-Y1k",
        "outputId": "1d18bb85-f74e-4870-92f0-5d45ea3a28ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/337.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m327.7/337.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.3/337.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Advanced RAG with LLM + Memory\"\"\"\n",
        "\n",
        "# 1. Load PDF and Split into Chunks\n",
        "from langchain.document_loaders import PDFMinerLoader, CSVLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 2. Set OpenAI API Key\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = key\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH')\n",
        "os.environ['LANGSMITH_TRACING']=\"true\"\n",
        "os.environ['LANGSMITH_PROJECT']=\"Advance RAG with LLM\"\n",
        "\n",
        "# 3. Create Vector Store (Qdrant + OpenAI Embeddings)\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_qdrant import Qdrant\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "# 4. Initialize LLM\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "\n",
        "# 5. Create Prompt Template\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Answer the Question:** Based *only* on the provided context, answer the following question.\n",
        "2.  **Cite Sources:** For each piece of information in your answer, cite the source from the context.\n",
        "3.  **Confidence Score:** Provide a confidence score (out of 100) for your answer.\n",
        "4.  **Reasoning:** Explain your reasoning for the answer and the confidence score.\n",
        "5.  **Rephrase the user's question:** - Given a user's conversational query, rewrite it to be more specific, keyword-rich, and optimized for searching in document-based question answering systems.\n",
        "6.  **Guardrails:**\n",
        "    *   Do not answer questions that are not related to the context.\n",
        "    *   If the answer is not found in the context, state that and do not provide an answer.\n",
        "    *   Do not generate any harmful, unethical, or offensive content.\n",
        "\n",
        "**Chat History:**\n",
        "{chat_history}\n",
        "\n",
        "**Context:**\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "**Original Question:** {question}\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Confidence Score:**\n",
        "\n",
        "**Reasoning:**\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# 6. Add Memory\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n",
        "# 9. Create Gradio Interface\n",
        "import gradio as gr\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "retrieval_chain = None\n",
        "last_file = None\n",
        "\n",
        "def answer(user_input, file):\n",
        "    global retrieval_chain, last_file\n",
        "    if file is None:\n",
        "        return \"Please upload a PDF or CSV file.\"\n",
        "\n",
        "    if file.name != last_file:\n",
        "        file_path = file.name\n",
        "        if file_path.endswith(\".pdf\"):\n",
        "            loader = PDFMinerLoader(file_path)\n",
        "        elif file_path.endswith(\".csv\"):\n",
        "            loader = CSVLoader(file_path)\n",
        "        else:\n",
        "            return \"Unsupported file type. Please upload a PDF or CSV file.\"\n",
        "\n",
        "        docs = loader.load()\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200, separators=[\"\\n\\n\", \"\\n\", \".\", \" \"])\n",
        "        documents = text_splitter.split_documents(docs)\n",
        "\n",
        "        qdrant = Qdrant.from_documents(\n",
        "            documents,\n",
        "            embedding,\n",
        "            location=\":memory:\",  # Local mode with in-memory storage only\n",
        "            collection_name=\"my_documents\",\n",
        "        )\n",
        "\n",
        "        retrieval_chain = ConversationalRetrievalChain.from_llm(\n",
        "            llm=llm,\n",
        "            retriever=qdrant.as_retriever(),\n",
        "            memory=memory,\n",
        "            combine_docs_chain_kwargs={\"prompt\": prompt}\n",
        "        )\n",
        "        last_file = file.name\n",
        "\n",
        "    response = retrieval_chain.invoke({\"question\": user_input})\n",
        "    return response['answer']\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=answer,\n",
        "    inputs=[gr.Textbox(placeholder=\"Ask a question about the document...\"), gr.File(label=\"Upload PDF or CSV\")],\n",
        "    outputs=\"text\",\n",
        "    title=\"RAG with Memory\",\n",
        "    description=\"Ask questions and carry on a conversation about the document.\"\n",
        ")\n",
        "\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-S4GAganNl0_",
        "outputId": "bba2d927-2034-43eb-a061-73a7f0db896c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3564080814.py:65: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://88ef5b91055923c3e5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://88ef5b91055923c3e5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/queueing.py\", line 626, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2235, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 1746, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/utils.py\", line 917, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3564080814.py\", line 94, in answer\n",
            "    qdrant = Qdrant.from_documents(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/vectorstores/base.py\", line 848, in from_documents\n",
            "    return cls.from_texts(texts, embedding, metadatas=metadatas, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_qdrant/vectorstores.py\", line 1317, in from_texts\n",
            "    qdrant = cls.construct_instance(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_qdrant/vectorstores.py\", line 1621, in construct_instance\n",
            "    partial_embeddings = embedding.embed_documents(texts[:1])\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_openai/embeddings/base.py\", line 590, in embed_documents\n",
            "    return self._get_len_safe_embeddings(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/langchain_openai/embeddings/base.py\", line 478, in _get_len_safe_embeddings\n",
            "    response = self.client.create(\n",
            "               ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/resources/embeddings.py\", line 132, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1256, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\", line 1044, in request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uMR0YQr-8gHm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}